{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Entity_Sentiment_Pairing_asba.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TbsdRAV5x-c"
      },
      "source": [
        "#!pip install pandas\r\n",
        "#!pip install numpy\r\n",
        "#!pip install nltk\r\n",
        "#!pip install stanfordnlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHJK76ZF6TTi"
      },
      "source": [
        "data_eng=pd.read_csv('/content/English_final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh-QK1mB8RU-",
        "outputId": "f2caac16-c9bc-405c-883d-76796a209a8b"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer \r\n",
        "import stanfordnlp\r\n",
        "import spacy\r\n",
        "# Make sure you have downloaded the StanfordNLP English model and other essential tools using,\r\n",
        "#stanfordnlp.download('en')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRro6kuTqRqV"
      },
      "source": [
        "def aspect_sentiment_analysis(txt, stop_words, nlp):\r\n",
        "    \r\n",
        "    txt = txt.lower() # LowerCasing the given Text\r\n",
        "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\r\n",
        "\r\n",
        "    fcluster = []\r\n",
        "    totalfeatureList = []\r\n",
        "    finalcluster = []\r\n",
        "    dic = {}\r\n",
        "\r\n",
        "    for line in sentList:\r\n",
        "        newtaggedList = []\r\n",
        "        txt_list = nltk.word_tokenize(line) # Splitting up into words\r\n",
        "        taggedList = nltk.pos_tag(txt_list) # Doing Part-of-Speech Tagging to each word\r\n",
        "\r\n",
        "        newwordList = []\r\n",
        "        flag = 0\r\n",
        "        for i in range(0,len(taggedList)-1):\r\n",
        "            if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"): # If two consecutive words are Nouns then they are joined together\r\n",
        "                newwordList.append(taggedList[i][0]+taggedList[i+1][0])\r\n",
        "                flag=1\r\n",
        "            else:\r\n",
        "                if(flag==1):\r\n",
        "                    flag=0\r\n",
        "                    continue\r\n",
        "                newwordList.append(taggedList[i][0])\r\n",
        "                if(i==len(taggedList)-2):\r\n",
        "                    newwordList.append(taggedList[i+1][0])\r\n",
        "\r\n",
        "        finaltxt = ' '.join(word for word in newwordList) \r\n",
        "        new_txt_list = nltk.word_tokenize(finaltxt)\r\n",
        "        wordsList = [w for w in new_txt_list if not w in stop_words]\r\n",
        "        taggedList = nltk.pos_tag(wordsList)\r\n",
        "\r\n",
        "        #print(wordsList)\r\n",
        "        #print(taggedList)\r\n",
        "        \r\n",
        "        doc = nlp(finaltxt) # Object of Stanford NLP Pipeleine\r\n",
        "        \r\n",
        "        # Getting the dependency relations betwwen the words\r\n",
        "        dep_node = []\r\n",
        "        for dep_edge in doc.sentences[0].dependencies:\r\n",
        "            dep_node.append([dep_edge[2].text, dep_edge[0].index, dep_edge[1]])\r\n",
        "        \r\n",
        "        #print(dep_node)    \r\n",
        "        \r\n",
        "        # Coverting it into appropriate format\r\n",
        "        for i in range(0, len(dep_node)):\r\n",
        "            if (int(dep_node[i][1]) != 0):\r\n",
        "              dep_node[i][1] = newwordList[ (int(dep_node[i][1])) - 1 ]\r\n",
        "                \r\n",
        "\r\n",
        "        featureList = []\r\n",
        "        categories = []\r\n",
        "        for i in taggedList:\r\n",
        "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\r\n",
        "                featureList.append(list(i)) # For features for each sentence\r\n",
        "                totalfeatureList.append(list(i)) # Stores the features of all the sentences in the text\r\n",
        "                categories.append(i[0])\r\n",
        "\r\n",
        "        for i in featureList:\r\n",
        "            filist = []\r\n",
        "            for j in dep_node:\r\n",
        "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\r\n",
        "                    if(j[0]==i[0]):\r\n",
        "                        filist.append(j[1])\r\n",
        "                    else:\r\n",
        "                        filist.append(j[0])\r\n",
        "            fcluster.append([i[0], filist])\r\n",
        "            \r\n",
        "    for i in totalfeatureList:\r\n",
        "        dic[i[0]] = i[1]\r\n",
        "    \r\n",
        "    for i in fcluster:\r\n",
        "        if(dic[i[0]]==\"NN\"):\r\n",
        "            finalcluster.append(i)\r\n",
        "        \r\n",
        "    return(finalcluster)\r\n",
        "    #'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT7amjxU9PLx"
      },
      "source": [
        "#import stanfordnlp\r\n",
        "#MODELS_DIR = '.'\r\n",
        "#stanfordnlp.download('en', MODELS_DIR) # Download the English models\r\n",
        "#nlp = stanfordnlp.Pipeline(models_dir=MODELS_DIR, treebank='en_ewt', use_gpu=True, pos_batch_size=3000)\r\n",
        "\r\n",
        "#nlp = stanfordnlp.Pipeline()\r\n",
        "\r\n",
        "nlp=spacy.load('en_core_web_sm')\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "txt = \"The Sound Quality is great but the battery life is very bad.\"\r\n",
        "\r\n",
        "print(aspect_sentiment_analysis(txt, stop_words, nlp))\r\n",
        "\r\n",
        "# Output: [['soundquality', ['great']], ['batterylife', ['bad']]]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifaCIRg-9Qdz"
      },
      "source": [
        "#import stanfordnlp\r\n",
        "\r\n",
        "#MODELS_DIR = '.'\r\n",
        "#stanfordnlp.download('en', MODELS_DIR) # Download the English models\r\n",
        "#nlp = stanfordnlp.Pipeline(models_dir=MODELS_DIR, treebank='en_ewt', use_gpu=True, pos_batch_size=3000) # Build the pipeline, specify part-of-speech processor's batch size\r\n",
        "#doc = nlp(\"The SoundQuality is great but the battery life is very bad\") # Run the pipeline on input text\r\n",
        "#doc.sentences[0].dependencies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDNe6B6s8mos"
      },
      "source": [
        "#data_eng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ye6xsvi8EdW"
      },
      "source": [
        "#MODELS_DIR = '.'\r\n",
        "#nlp = stanfordnlp.Pipeline(models_dir=MODELS_DIR, treebank='en_ewt', use_gpu=True, pos_batch_size=3000) \r\n",
        "#nlp=stanfordnlp.Pipeline()\r\n",
        "\r\n",
        "txt = data_eng['Tweet'][0]\r\n",
        "nlp=spacy.load('en_core_web_sm')\r\n",
        "doc = nlp(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LybvM9i7JqI7"
      },
      "source": [
        "#for token in doc:\r\n",
        "#  print(token, token.dep_, token.head.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JInCxHKk6Rik"
      },
      "source": [
        "def Noun_dependancy_pair(txt):\r\n",
        "  txt = txt.lower() # LowerCasing the given Text\r\n",
        "  sentList = nltk.sent_tokenize(txt)\r\n",
        "  doc = nlp(txt)\r\n",
        "  #print(sentList) # Splitting the text into sentences\r\n",
        "  fcluster = []\r\n",
        "  totalfeatureList = []\r\n",
        "  finalcluster = []\r\n",
        "  dic = {}\r\n",
        "  for line in sentList:\r\n",
        "    newtaggedList = []\r\n",
        "    txt_list = nltk.word_tokenize(line) #[word for word in line.split()]#nltk.word_tokenize(line) # Splitting up into word\r\n",
        "    taggedList = nltk.pos_tag(txt_list) # Doing Part-of-Speech Tagging to each word\r\n",
        "  #print(taggedList)\r\n",
        "  newwordList = []\r\n",
        "  flag = 0\r\n",
        "  for i in range(0,len(taggedList)-1):\r\n",
        "    if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"): \r\n",
        "      # If two consecutive words are Nouns then they are joined togethe\r\n",
        "      newwordList.append(taggedList[i][0]+taggedList[i+1][0])\r\n",
        "      flag=1\r\n",
        "    else:\r\n",
        "      if(flag==1):\r\n",
        "        flag=0\r\n",
        "        continue\r\n",
        "    newwordList.append(taggedList[i][0])\r\n",
        "    if(i==len(taggedList)-2):\r\n",
        "      newwordList.append(taggedList[i+1][0])\r\n",
        "  #print(newwordList)\r\n",
        "  finaltxt = ' '.join(word for word in newwordList) \r\n",
        "  #print(finaltxt)\r\n",
        "  \r\n",
        "  new_txt_list = nltk.word_tokenize(finaltxt) #[word for word in finaltxt.split()] #nltk.word_tokenize(finaltxt)\r\n",
        "  #print(new_txt_list)\r\n",
        "  wordsList = [w for w in new_txt_list if not w in stop_words]\r\n",
        "  #print(wordsList)\r\n",
        "  taggedList = nltk.pos_tag(wordsList)\r\n",
        "  #print(taggedList)\r\n",
        "  dep_node = []\r\n",
        "  for token in doc:\r\n",
        "    #print(nltk.pos_tag(nltk.word_tokenize(txt)))\r\n",
        "    dep_node.append([token.text, token.head.text, token.dep_])\r\n",
        "  #print(dep_node)\r\n",
        "  featureList = []\r\n",
        "  categories = []\r\n",
        "  for i in taggedList:\r\n",
        "    if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\r\n",
        "      featureList.append(list(i)) # For features for each sentence\r\n",
        "      totalfeatureList.append(list(i)) # Stores the features of all the sentences in the text\r\n",
        "      categories.append(i[0])\r\n",
        "  \r\n",
        "  for i in featureList:\r\n",
        "    filist = []\r\n",
        "    for j in dep_node:\r\n",
        "      if(((j[0] in i[0]) or (j[1] in i[0]))):\r\n",
        "        if(j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"]):\r\n",
        "          if(j[0] in i[0]):\r\n",
        "            filist.append(j[1])\r\n",
        "          else:\r\n",
        "            filist.append(j[0])\r\n",
        "    fcluster.append([i[0], filist])\r\n",
        "  \r\n",
        "  for i in totalfeatureList:\r\n",
        "    dic[i[0]] = i[1]\r\n",
        "  \r\n",
        "  ent_lis=[]\r\n",
        "  for ent in doc.ents:\r\n",
        "    ent_lis.append(ent.text)  \r\n",
        "  for i in fcluster:\r\n",
        "    if(dic[i[0]]==\"NN\"):\r\n",
        "      finalcluster.append(i)  \r\n",
        "  return(finalcluster)       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fZOfAi7lQuW"
      },
      "source": [
        "#for ent in doc.ents:\r\n",
        "#  print(ent.text,ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8TOgKRLN3vS"
      },
      "source": [
        "finalcluster= Noun_dependancy_pair(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKGmX6hhNDpx",
        "outputId": "7c46e691-2a67-4b7a-8c52-44c7bb822b14"
      },
      "source": [
        "finalcluster"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['samsung', ['follow', 'updates']],\n",
              " ['addition', ['series']],\n",
              " ['seriesword', ['addition', 'word', 'making']],\n",
              " ['series', ['addition', 'word']],\n",
              " ['galaxy', ['variant', 'upcoming', 'updates']],\n",
              " ['batterycharger', ['updates', 'awesome', 'updates']],\n",
              " ['battery', ['awesome', 'updates']],\n",
              " ['specstech', ['updates', 'updates']],\n",
              " ['specs', ['updates']],\n",
              " ['backhandindexpointingrightsmartphone', ['updates', 'mobilephone']],\n",
              " ['backhandindexpointingright', ['updates', 'mobilephone']],\n",
              " ['mobilephonemobilephoto',\n",
              "  ['backhandindexpointingright', 'mobilephone', 'follow']],\n",
              " ['mobilephone', ['backhandindexpointingright', 'mobilephone', 'follow']],\n",
              " ['mobilephoto', ['mobilephone']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6HoFjX_CjsS",
        "outputId": "a7ad941e-bd28-4bcc-c4e4-9f22ed7b171d"
      },
      "source": [
        "#dep_node = []\r\n",
        "#for dep_edge in doc.sentences[0].dependencies:\r\n",
        "#  dep_node.append([dep_edge[2].text, dep_edge[0].index, dep_edge[1]])\r\n",
        "#print(dep_node)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['The', '3', 'det'], ['Sound', '3', 'compound'], ['Quality', '5', 'nsubj'], ['is', '5', 'cop'], ['great', '0', 'root'], ['but', '12', 'cc'], ['the', '9', 'det'], ['battery', '9', 'compound'], ['life', '12', 'nsubj'], ['is', '12', 'cop'], ['very', '12', 'advmod'], ['bad', '5', 'conj']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpTq9e-34KRa"
      },
      "source": [
        "# Coverting it into appropriate format\r\n",
        "#for i in range(0, len(dep_node)):\r\n",
        "#  if (int(dep_node[i][1])!= 0):\r\n",
        "#    #print(int(dep_node[i][1]))  \r\n",
        "#    dep_node[i][1] = newwordList[ (int(dep_node[i][1]))]\r\n",
        "                              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbizLzFQ8eB8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxFu4EwHaHCQ"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "afinn_wl_url = ('https://raw.githubusercontent.com'\r\n",
        "                '/fnielsen/afinn/master/afinn/data/AFINN-111.txt')\r\n",
        "\r\n",
        "afinn_wl_df = pd.read_csv(afinn_wl_url,\r\n",
        "                          header=None, # no column names\r\n",
        "                          sep='\\t',  # tab sepeated\r\n",
        "                          names=['term', 'value']) #new column names\r\n",
        "\r\n",
        "seed = 808 # seed for sample so results are stable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olwt5TBnaH8V"
      },
      "source": [
        "pos=[]\r\n",
        "neg=[]\r\n",
        "for i in range(len(afinn_wl_df)):\r\n",
        "  if(afinn_wl_df['value'][i]>=0):\r\n",
        "    pos.append(afinn_wl_df['term'][i])\r\n",
        "  else:\r\n",
        "    neg.append(afinn_wl_df['term'][i])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl3f00vFaL8P"
      },
      "source": [
        "# create a list of globally defined positive and negative words to identify sentiment\r\n",
        "# sentiment score based on the laxicon neg, pos words\r\n",
        "def feature_sentiment(sentence, pos=pos, neg=neg):\r\n",
        "    sent_dict = dict()\r\n",
        "    sentence = nlp(sentence)\r\n",
        "    opinion_words = neg + pos\r\n",
        "    debug = 0\r\n",
        "    for token in sentence:\r\n",
        "        #sentiment=0.01\r\n",
        "        #print(token.head)\r\n",
        "        # check if the word is an opinion word, then assign sentiment\r\n",
        "        if token.text in opinion_words:\r\n",
        "            sentiment = 1 if token.text in pos else -1\r\n",
        "            # if target is an adverb modifier (i.e. pretty, highly, etc.)\r\n",
        "            # but happens to be an opinion word, ignore and pass\r\n",
        "            if (token.dep_ == \"advmod\"):\r\n",
        "              continue\r\n",
        "            elif (token.dep_ == \"amod\"):\r\n",
        "              sent_dict[token.head.text] = sentiment\r\n",
        "          # for opinion words that are adjectives, adverbs, verbs...\r\n",
        "            else:\r\n",
        "              for child in token.children:\r\n",
        "                # if there's a adj modifier (i.e. very, pretty, etc.) add more weight to sentiment\r\n",
        "                # This could be better updated for modifiers that either positively or negatively emphasize\r\n",
        "                if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\r\n",
        "                  sentiment *= 1.5\r\n",
        "                  # check for negation words and flip the sign of sentiment\r\n",
        "                if child.dep_ == \"neg\":\r\n",
        "                  sentiment *= -1\r\n",
        "              for child in token.children:\r\n",
        "                # if verb, check if there's a direct object\r\n",
        "                if (token.pos_ == \"VERB\") & (child.dep_ == \"dobj\"):                        \r\n",
        "                  sent_dict[child.text] = sentiment\r\n",
        "                  # check for conjugates (a AND b), then add both to dictionary\r\n",
        "                  subchildren = []\r\n",
        "                  conj = 0\r\n",
        "                  for subchild in child.children:\r\n",
        "                    if subchild.text == \"and\":\r\n",
        "                      conj=1\r\n",
        "                    if (conj == 1) and (subchild.text != \"and\"):\r\n",
        "                      subchildren.append(subchild.text)\r\n",
        "                      conj = 0\r\n",
        "                  for subchild in subchildren:\r\n",
        "                    sent_dict[subchild] = sentiment\r\n",
        "\r\n",
        "                # check for negation\r\n",
        "              for child in token.head.children:\r\n",
        "                noun = \"\"\r\n",
        "                if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\r\n",
        "                  sentiment *= 1.5\r\n",
        "                  # check for negation words and flip the sign of sentiment\r\n",
        "                if (child.dep_ == \"neg\"): \r\n",
        "                  sentiment *= -1\r\n",
        "                \r\n",
        "                # check for nouns\r\n",
        "              for child in token.head.children:\r\n",
        "                noun = \"\"\r\n",
        "                if (child.pos_ == \"NOUN\") and (child.text not in sent_dict):\r\n",
        "                  noun = child.text\r\n",
        "                  # Check for compound nouns\r\n",
        "                  for subchild in child.children:\r\n",
        "                    if subchild.dep_ == \"compound\":\r\n",
        "                      noun = subchild.text + \" \" + noun\r\n",
        "                    sent_dict[noun] = sentiment\r\n",
        "                    debug += 1\r\n",
        "    return sent_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWUamDNUihdl"
      },
      "source": [
        "sent_dict=feature_sentiment(txt,pos,neg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izZIAVvbim3J"
      },
      "source": [
        "def entity_pair(text):\r\n",
        "  #print(text)\r\n",
        "  doc=nlp(text)\r\n",
        "  lis=[]\r\n",
        "  for ent in doc.ents:\r\n",
        "    #print(ent.label_, ent.text)\r\n",
        "    if(ent.label_=='ORG'):\r\n",
        "      lis.append(ent.text.lower())\r\n",
        "  \r\n",
        "  return(lis)  \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRJlrQMCqCut"
      },
      "source": [
        "#entity_pair(data_eng['Tweet'][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DASwYI4SinsO"
      },
      "source": [
        "data_eng['Senti_Dict']=data_eng['Tweet'].apply(lambda x: feature_sentiment(x,pos,neg))\r\n",
        "data_eng['Noun_Dependancy_Pair']= data_eng['Tweet'].apply(lambda x: Noun_dependancy_pair(x))\r\n",
        "data_eng['Entity_list']=data_eng['Tweet'].apply(lambda x: entity_pair(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A_B2loZnxIU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkWQEe-dny_A",
        "outputId": "a17215cd-ac85-4779-e795-6db85371348e"
      },
      "source": [
        "finalcluster"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['samsung', ['follow', 'updates']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90Tj90bvoZnG",
        "outputId": "b3f38ffa-e027-43f3-f47f-f756196b1c24"
      },
      "source": [
        "sent_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'battery': 1, 'updates': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXuDF6O0rNsM"
      },
      "source": [
        "def entity_sentiment_pairs(text,pos=pos, neg=neg):\r\n",
        "  finalcluster=Noun_dependancy_pair(text)\r\n",
        "  senti_dict= feature_sentiment(text,pos,neg)\r\n",
        "  entity_list= entity_pair(text)\r\n",
        "  print(entity_list)\r\n",
        "  print(finalcluster)\r\n",
        "  print(senti_dict)\r\n",
        "  \r\n",
        "  filtered_entities=[]\r\n",
        "  filtered_pairs=[]\r\n",
        "  #all_noun_list=[]\r\n",
        "  \r\n",
        "  for i in range(len(finalcluster)):\r\n",
        "    #all_noun_list.append(finalcluster[i][0])\r\n",
        "    if(finalcluster[i][0] in entity_list):\r\n",
        "      filtered_entities.append(finalcluster[i][0])\r\n",
        "      filtered_pairs.append(finalcluster[i][1])\r\n",
        "  \r\n",
        "  entity_dict={}\r\n",
        "  for i in range(len(filtered_entities)):\r\n",
        "    senti=0\r\n",
        "    for j in filtered_pairs[i]:\r\n",
        "      if(j in senti_dict.keys()):\r\n",
        "        senti+=senti_dict[j]\r\n",
        "    if(senti >=0):\r\n",
        "      entity_dict[filtered_entities[i]]='positive'\r\n",
        "    else:\r\n",
        "      entity_dict[filtered_entities[i]]='negative'\r\n",
        "\r\n",
        "  return(entity_dict)        \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmI0I-HSsSGf",
        "outputId": "970dc2db-732f-48ef-8008-ebc02397ccae"
      },
      "source": [
        "entity_sentiment_pairs('Apple is great,while Samsung sucks')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['apple', 'samsung']\n",
            "[['apple', ['is']]]\n",
            "{}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apple': 'positive'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qke4eyM2sWoX"
      },
      "source": [
        "data_eng['Entity_Sentiment_Pair_Dict']=data_eng['Tweet'].apply(lambda x: entity_sentiment_pairs(x,pos,neg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2V3nudTwdOy"
      },
      "source": [
        "data_eng.to_csv('English_Results.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shlInR2JwkrY"
      },
      "source": [
        "data_eng['Entity_Sentiment_Pair_Dict'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfgXBHmfzJKt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxWcreb4OK1X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "8PCIxV-7OK5l",
        "outputId": "0f5ea5c0-ee98-4e32-d10a-155a4a8e5e07"
      },
      "source": [
        "#!pip install bella-tdsa\r\n",
        "from bella-tdsa import helper\r\n",
        "from bella-tdsa.models.target import TargetDep\r\n",
        "\r\n",
        "target_dep = helper.download_model(TargetDep, 'SemEval 14 Restaurant')\r\n",
        "test_example_multi = [{'text' : 'This bread is tasty but the sauce is too rich', 'target': 'sauce', \r\n",
        "                       'spans': [(28, 33)]}]\r\n",
        "\r\n",
        "target_dep.predict(test_example_multi)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-f3b009ebce82>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from bella-tdsa import helper\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txWbFSWzONF0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}