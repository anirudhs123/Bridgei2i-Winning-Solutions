{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Noun_Identification_ABSA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZmK4zMo8kt6"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWsSNkwu83Zb"
      },
      "source": [
        "data_eng=pd.read_csv('/content/English_final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_hr--IC8-nA"
      },
      "source": [
        "data_eng['Id']=data_eng['Unnamed: 0']\r\n",
        "data_eng['Cleaned_tweet']=data_eng['Tweet']\r\n",
        "data_eng=data_eng.drop(['Unnamed: 0','Tweet'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Q5ZqLr8_oI"
      },
      "source": [
        "#data_res=pd.read_csv('/content/Results.csv')\r\n",
        "#data_res=data_res.drop(['Unnamed: 0'],axis=1)\r\n",
        "#pd.concat([data_eng,data_res],axis=1).dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6P9NjsdKMAv"
      },
      "source": [
        "import spacy\r\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icnnfJCT92xM"
      },
      "source": [
        "nlp=spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU1Wiw9a9cLL"
      },
      "source": [
        "def show_ents(doc): \r\n",
        "  lis=[]\r\n",
        "  if doc.ents: \r\n",
        "    for ent in doc.ents: \r\n",
        "      if(ent.label_=='ORG' or ent.label_=='PRODUCT'):\r\n",
        "        if(ent.text.lower() not in lis):\r\n",
        "          lis.append(ent.text.lower())\r\n",
        "  return(lis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSkx8fgs-Bod"
      },
      "source": [
        "aspect_lis=[]\r\n",
        "for i in range(len(data_eng)):\r\n",
        "  aspect_lis+=show_ents(nlp(data_eng['Cleaned_tweet'][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DuSb-7eO_Ho"
      },
      "source": [
        "#Unique aspects\r\n",
        "aspect_entity_lis=[]\r\n",
        "for word in aspect_lis:\r\n",
        "  if(word not in aspect_entity_lis):\r\n",
        "    aspect_entity_lis.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0TzcCD7LXMH"
      },
      "source": [
        "def pos_tagging(data):\r\n",
        "    #print(\"pos tagging\")\r\n",
        "    req_tag = ['NN','NNP','NNS']\r\n",
        "    extracted_words = []\r\n",
        "    i = 0\r\n",
        "    try:\r\n",
        "        for x in data['Cleaned_tweet'].values:\r\n",
        "            doc =nlp(x)\r\n",
        "            for token in doc:\r\n",
        "                i += 1\r\n",
        "                if token.tag_ in req_tag and token.shape_ != 'x' and token.shape_ != 'xx' and token.shape_ != 'xxx':\r\n",
        "                  if(token.lemma_ not in extracted_words):\r\n",
        "                    extracted_words.append(token.lemma_)\r\n",
        "        return extracted_words\r\n",
        "    except Exception as e:\r\n",
        "        return extracted_words\r\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYMr5pdrMQxi"
      },
      "source": [
        "#Collecting all the Nouns\r\n",
        "extract_words = pos_tagging(data_eng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI_GX5OQV0rz"
      },
      "source": [
        "from gensim.models import word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5F9EeXoXB_P"
      },
      "source": [
        "#!pip install nltk\r\n",
        "#!pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85JadXfQe9jL"
      },
      "source": [
        "corpus=[]\r\n",
        "for i in range(len(data_eng)):\r\n",
        "  lis=[]\r\n",
        "  for word in data_eng['Cleaned_tweet'][i].split():\r\n",
        "    lis.append(word)\r\n",
        "  corpus.append(lis)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLJMye3NfY0a"
      },
      "source": [
        "#corpus=data_eng['Cleaned_tweet'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xf2FPW6iHk0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOqnyGumXF8w"
      },
      "source": [
        "import gensim \r\n",
        "from gensim.models import Word2Vec\r\n",
        "model_wiki=gensim.models.Word2Vec(corpus, size = 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adImivyQWjzY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IBw4-rJQTEM"
      },
      "source": [
        "def word2vec(corpus):\r\n",
        "    terms = corpus\r\n",
        "    try:\r\n",
        "      # removal of words which not present in the word2vec model vocabulary. (wrongly spelled)\r\n",
        "      filtered_terms = []\r\n",
        "      for i in range(len(terms)):\r\n",
        "        for word in terms[i]:\r\n",
        "          filtered_terms.append(word) \r\n",
        "        \r\n",
        "\r\n",
        "        \r\n",
        "        #converting words into vector\r\n",
        "      vector_of_terms = []\r\n",
        "      for x in range(len(filtered_terms)):\r\n",
        "        vector_of_terms.append(model_wiki.wv[filtered_terms[x]])\r\n",
        "      return vector_of_terms,filtered_terms\r\n",
        "    \r\n",
        "    except Exception as e:\r\n",
        "      return abort(Response(\r\n",
        "            json.dumps({'status_code': 400, 'success': False, 'message': 'Something went wrong'}),\r\n",
        "            mimetype=\"application/json\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbStGWIcnIxU"
      },
      "source": [
        "vector_of_terms = []\r\n",
        "for x in range(len(vocab)):\r\n",
        "  vector_of_terms.append(model_wiki.wv[vocab[x]])\r\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaQCCFd0pKZ_"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "afinn_wl_url = ('https://raw.githubusercontent.com'\r\n",
        "                '/fnielsen/afinn/master/afinn/data/AFINN-111.txt')\r\n",
        "\r\n",
        "afinn_wl_df = pd.read_csv(afinn_wl_url,\r\n",
        "                          header=None, # no column names\r\n",
        "                          sep='\\t',  # tab sepeated\r\n",
        "                          names=['term', 'value']) #new column names\r\n",
        "\r\n",
        "seed = 808 # seed for sample so results are stable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui4d9WzRp4SY"
      },
      "source": [
        "pos=[]\r\n",
        "neg=[]\r\n",
        "for i in range(len(afinn_wl_df)):\r\n",
        "  if(afinn_wl_df['value'][i]>=0):\r\n",
        "    pos.append(afinn_wl_df['term'][i])\r\n",
        "  else:\r\n",
        "    neg.append(afinn_wl_df['term'][i])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFYwiUsqpwXO"
      },
      "source": [
        "#afinn_wl_df['value'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aAAFacqM1e8"
      },
      "source": [
        "# create a list of globally defined positive and negative words to identify sentiment\r\n",
        "# sentiment score based on the laxicon neg, pos words\r\n",
        "def feature_sentiment(sentence, pos, neg):\r\n",
        "    '''\r\n",
        "    input: dictionary and sentence\r\n",
        "    function: appends dictionary with new features if the feature\r\n",
        "              did not exist previously,then updates sentiment to\r\n",
        "              each of the new or existing features\r\n",
        "    output: updated dictionary\r\n",
        "    '''\r\n",
        "    sent_dict = dict()\r\n",
        "    sentence = nlp(sentence)\r\n",
        "    opinion_words = neg + pos\r\n",
        "    debug = 0\r\n",
        "    for token in sentence:\r\n",
        "        #sentiment=0.01\r\n",
        "        #print(token.head)\r\n",
        "        # check if the word is an opinion word, then assign sentiment\r\n",
        "        if token.text in opinion_words:\r\n",
        "            sentiment = 1 if token.text in pos else -1\r\n",
        "            # if target is an adverb modifier (i.e. pretty, highly, etc.)\r\n",
        "            # but happens to be an opinion word, ignore and pass\r\n",
        "            if (token.dep_ == \"advmod\"):\r\n",
        "              continue\r\n",
        "            elif (token.dep_ == \"amod\"):\r\n",
        "              sent_dict[token.head.text] = sentiment\r\n",
        "          # for opinion words that are adjectives, adverbs, verbs...\r\n",
        "            else:\r\n",
        "              for child in token.children:\r\n",
        "                # if there's a adj modifier (i.e. very, pretty, etc.) add more weight to sentiment\r\n",
        "                # This could be better updated for modifiers that either positively or negatively emphasize\r\n",
        "                if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\r\n",
        "                  sentiment *= 1.5\r\n",
        "                  # check for negation words and flip the sign of sentiment\r\n",
        "                if child.dep_ == \"neg\":\r\n",
        "                  sentiment *= -1\r\n",
        "              for child in token.children:\r\n",
        "                # if verb, check if there's a direct object\r\n",
        "                if (token.pos_ == \"VERB\") & (child.dep_ == \"dobj\"):                        \r\n",
        "                  sent_dict[child.text] = sentiment\r\n",
        "                  # check for conjugates (a AND b), then add both to dictionary\r\n",
        "                  subchildren = []\r\n",
        "                  conj = 0\r\n",
        "                  for subchild in child.children:\r\n",
        "                    if subchild.text == \"and\":\r\n",
        "                      conj=1\r\n",
        "                    if (conj == 1) and (subchild.text != \"and\"):\r\n",
        "                      subchildren.append(subchild.text)\r\n",
        "                      conj = 0\r\n",
        "                  for subchild in subchildren:\r\n",
        "                    sent_dict[subchild] = sentiment\r\n",
        "\r\n",
        "                # check for negation\r\n",
        "              for child in token.head.children:\r\n",
        "                noun = \"\"\r\n",
        "                if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\r\n",
        "                  sentiment *= 1.5\r\n",
        "                  # check for negation words and flip the sign of sentiment\r\n",
        "                if (child.dep_ == \"neg\"): \r\n",
        "                  sentiment *= -1\r\n",
        "                \r\n",
        "                # check for nouns\r\n",
        "              for child in token.head.children:\r\n",
        "                noun = \"\"\r\n",
        "                if (child.pos_ == \"NOUN\") and (child.text not in sent_dict):\r\n",
        "                  noun = child.text\r\n",
        "                  # Check for compound nouns\r\n",
        "                  for subchild in child.children:\r\n",
        "                    if subchild.dep_ == \"compound\":\r\n",
        "                      noun = subchild.text + \" \" + noun\r\n",
        "                    sent_dict[noun] = sentiment\r\n",
        "                    debug += 1\r\n",
        "    return sent_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgtJi3iVQE-G",
        "outputId": "f92f6dd8-b0ac-4ee1-d2be-588d693cbf9e"
      },
      "source": [
        "# example \r\n",
        "tweet = data_eng['Cleaned_tweet'][1]\r\n",
        "print (feature_sentiment(tweet, pos, neg))\r\n",
        "## Output: {'food': 1, 'service': -1}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'cameras': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TMytWOlGeyUm",
        "outputId": "5dbd38e8-e3a4-4a5f-c816-a05f0acafb1c"
      },
      "source": [
        "data_eng['Cleaned_tweet'][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Want hands GalaxyS21 pleadingface Samsung GalaxyS21 Ultra 5G premium Smartphone yet strong focus cameras'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uIjjEi2sD1n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReVted6Aybt0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_fG_Zbx4asL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19Jb4CRR4avW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFY3SNGe4axz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P1umX_64a0q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEhorg5z4a31"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}