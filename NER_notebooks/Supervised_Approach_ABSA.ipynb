{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supervised_Approach_ASBA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aacJ3As4ZiE4"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "\r\n",
        "from sklearn.impute import SimpleImputer\r\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\r\n",
        "from sklearn.neural_network import MLPRegressor, MLPClassifier\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\r\n",
        "import re\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from sklearn.svm import SVC\r\n",
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpFouUl0ZOcg"
      },
      "source": [
        "def gaussianNaiveBayes(X, Y):\r\n",
        "\r\n",
        "    # Split in train and test\r\n",
        "    xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.25, random_state=0)\r\n",
        "    classifier = GaussianNB()\r\n",
        "    classifier.fit(xTrain, yTrain)\r\n",
        "\r\n",
        "    # Make predictions\r\n",
        "    yPred = classifier.predict(xTest)\r\n",
        "\r\n",
        "    # Confusion Matrix and accuracy\r\n",
        "    matrix = confusion_matrix(y_true=yTest, y_pred=yPred)\r\n",
        "    accuracy = accuracy_score(yPred, yTest)\r\n",
        "\r\n",
        "    # Precision, Recall and F-Score\r\n",
        "    fScore = f1_score(yTest, yPred, average=\"macro\")\r\n",
        "    precision = precision_score(yTest, yPred, average=\"macro\")\r\n",
        "    recall = recall_score(yTest, yPred, average=\"macro\")\r\n",
        "    report = classification_report(yTest, yPred)\r\n",
        "\r\n",
        "\r\n",
        "    return [matrix, accuracy, fScore, precision, recall, report]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def MultiLayerPerceptron(X, Y):\r\n",
        "\r\n",
        "    # Split in train and test\r\n",
        "    xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.25, random_state=0)\r\n",
        "\r\n",
        "    # Fit the classifier\r\n",
        "    classifier = MLPClassifier(alpha=10.0 ** -1, hidden_layer_sizes=(100,150), max_iter=100)\r\n",
        "    classifier.fit(xTrain, yTrain)\r\n",
        "\r\n",
        "    # Make predictions\r\n",
        "    yPred = classifier.predict(xTest)\r\n",
        "\r\n",
        "    # Confusion Matrix and accuracy\r\n",
        "    matrix = confusion_matrix(y_true=yTest, y_pred=yPred)\r\n",
        "    accuracy = accuracy_score(yPred, yTest)\r\n",
        "\r\n",
        "    # Precision, Recall and F-Score\r\n",
        "    fScore = f1_score(yTest, yPred, average=\"macro\")\r\n",
        "    precision = precision_score(yTest, yPred, average=\"macro\")\r\n",
        "    recall = recall_score(yTest, yPred, average=\"macro\")\r\n",
        "    report = classification_report(yTest, yPred)\r\n",
        "    return [matrix, accuracy, fScore, precision, recall, report]\r\n",
        "\r\n",
        "\r\n",
        "def SVM(X, Y):\r\n",
        "\r\n",
        "    # Split in train and test\r\n",
        "    xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.25, random_state=0)\r\n",
        "\r\n",
        "    # Cross Validation\r\n",
        "    # print(\"SVM Cross Validation!\")\r\n",
        "    # svm = SVC()\r\n",
        "    # parameters = {'kernel': ['linear'], 'C': (1, 10)\r\n",
        "    #     , 'gamma': ('auto', 'scale')\r\n",
        "    #     # , 'gamma': (0.001, 0.01, 0.1, 1, 2, 3, 'auto')\r\n",
        "    #     ,'decision_function_shape': ('ovo', 'ovr')\r\n",
        "    #     # , 'shrinking': (True, False)\r\n",
        "    #               }\r\n",
        "    # clf = GridSearchCV(svm, parameters,scoring=\"accuracy\",\r\n",
        "    #                           cv=10,\r\n",
        "    #                           n_jobs=8, verbose=10)\r\n",
        "    # clf.fit(xTrain, yTrain)\r\n",
        "    # bestAccuracy = clf.best_score_\r\n",
        "    # bestParameters = clf.best_params_\r\n",
        "    # print(\"The best parameters for MLP model are :\\n{}\\n\".format(bestParameters))\r\n",
        "    # print(bestAccuracy)\r\n",
        "\r\n",
        "\r\n",
        "    # {'C': 1, 'decision_function_shape': 'ovo', 'gamma': 'auto', 'kernel': 'linear'}\r\n",
        "    # # Fit the classifier\r\n",
        "    classifier =SVC(C=1, kernel='linear', decision_function_shape='ovo', gamma='auto')\r\n",
        "    classifier.fit(xTrain, yTrain)\r\n",
        "\r\n",
        "    # Make predictions\r\n",
        "    yPred = classifier.predict(xTest)\r\n",
        "\r\n",
        "    # Confusion Matrix and accuracy\r\n",
        "    matrix = confusion_matrix(y_true=yTest, y_pred=yPred)\r\n",
        "    accuracy = accuracy_score(yPred, yTest)\r\n",
        "\r\n",
        "    # Precision, Recall and F-Score\r\n",
        "    fScore = f1_score(yTest, yPred, average=\"macro\")\r\n",
        "    precision = precision_score(yTest, yPred, average=\"macro\")\r\n",
        "    recall = recall_score(yTest, yPred, average=\"macro\")\r\n",
        "    report = classification_report(yTest, yPred)\r\n",
        "    return [matrix, accuracy, fScore, precision, recall, report]\r\n",
        "\r\n",
        "def trainBestClassifier(X, Y):\r\n",
        "\r\n",
        "\r\n",
        "    # {'C': 1, 'decision_function_shape': 'ovo', 'gamma': 'auto', 'kernel': 'linear'}\r\n",
        "    # Fit the classifier\r\n",
        "    classifier =SVC(C=1, kernel='linear', decision_function_shape='ovo', gamma='auto')\r\n",
        "    classifier.fit(X, Y)\r\n",
        "\r\n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZNAk861ZU8k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDg2-H6la82y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV8JdkMGawoa"
      },
      "source": [
        "def aspectAnalysis(df, output=False):\r\n",
        "\r\n",
        "    count = 0\r\n",
        "    filteredWordsList = []\r\n",
        "\r\n",
        "    for row, aspect in zip(df['tagged_words'], df['aspect_term']):\r\n",
        "\r\n",
        "        # Variables to store left and right windows\r\n",
        "        leftPart = []\r\n",
        "        rightPart = []\r\n",
        "\r\n",
        "        aspectSplit = word_tokenize(aspect)\r\n",
        "        aspectTermsLen = len(aspectSplit)\r\n",
        "\r\n",
        "        # Can change the window size\r\n",
        "        windowSize = 10\r\n",
        "\r\n",
        "        # Find the aspect term's first word's index in row\r\n",
        "        for i in range(len(row)):\r\n",
        "            if aspectSplit[0] == row[i][0]:\r\n",
        "                # print('Matched Word is ', row[i][0])\r\n",
        "                aspectIndex = i\r\n",
        "                break\r\n",
        "\r\n",
        "        # Variable to decrement the window size dynamically\r\n",
        "        # if sentence does not have enough words to fit in the window\r\n",
        "        windowNotAssigned = True\r\n",
        "\r\n",
        "        while windowNotAssigned:\r\n",
        "\r\n",
        "            # Best Case : When the window fits both left and right sides\r\n",
        "            if (aspectIndex - (windowSize//2) >= 0) and (aspectIndex + aspectTermsLen + (windowSize - (windowSize//2)) < len(row)):\r\n",
        "                leftPart = row[(aspectIndex - (windowSize//2)) : aspectIndex]\r\n",
        "                rightPart = row[aspectIndex + aspectTermsLen : (aspectIndex + (windowSize - (windowSize//2)))]\r\n",
        "\r\n",
        "                windowNotAssigned = False\r\n",
        "\r\n",
        "            # Case when right side doesn't fit in window\r\n",
        "            elif (aspectIndex - (windowSize//2) >= 0) and (aspectIndex + aspectTermsLen + (windowSize - (windowSize//2)) >= len(row)):\r\n",
        "                rightPart = row[aspectIndex + aspectTermsLen : ]\r\n",
        "                missingRightLen = (windowSize//2) - len(rightPart)\r\n",
        "\r\n",
        "                # Check if we can accomodate the missing right part on left side\r\n",
        "                if (aspectIndex - (windowSize//2) - missingRightLen) >= 0:\r\n",
        "                    leftPart = row[(aspectIndex - (windowSize//2) - missingRightLen) : aspectIndex]\r\n",
        "                else:\r\n",
        "                    leftPart = row[: aspectIndex]\r\n",
        "\r\n",
        "                windowNotAssigned = False\r\n",
        "\r\n",
        "            # Case when left side doesn't fit the window\r\n",
        "            elif (aspectIndex - (windowSize//2) < 0) and (aspectIndex + aspectTermsLen + (windowSize - (windowSize//2)) < len(row)):\r\n",
        "                leftPart = row[0 : aspectIndex]\r\n",
        "                missingLeftLen = (windowSize//2) - len(leftPart)\r\n",
        "\r\n",
        "                # Check if we can accomodate the missing left part on right side\r\n",
        "                if (aspectIndex + aspectTermsLen + (windowSize - (windowSize//2)) + missingLeftLen) < len(row):\r\n",
        "                    rightPart = row[aspectIndex + aspectTermsLen : (aspectIndex + (windowSize - (windowSize//2)) + missingLeftLen)]\r\n",
        "                else:\r\n",
        "                    rightPart = row[aspectIndex + aspectTermsLen :]\r\n",
        "\r\n",
        "                windowNotAssigned = False\r\n",
        "\r\n",
        "            # Worst case : When not enough words on both left and right sides of aspect term\r\n",
        "            # Decrement the window size and try again\r\n",
        "            else:\r\n",
        "\r\n",
        "                windowSize -= 1\r\n",
        "\r\n",
        "        filteredWords = leftPart + rightPart\r\n",
        "        # print(count)\r\n",
        "        # print(filteredWords)\r\n",
        "        filteredWordsList.append(filteredWords)\r\n",
        "        count += 1\r\n",
        "\r\n",
        "    # Create a column with the important words around the aspect term with the window size\r\n",
        "    filteredWordsList = pd.Series(filteredWordsList)\r\n",
        "    df['important_words'] = filteredWordsList.values\r\n",
        "\r\n",
        "    # Split the words as sentence in df[]\r\n",
        "    def splitWords(x):\r\n",
        "\r\n",
        "        s = [i[0] for i in x]\r\n",
        "        return ' '.join(s)\r\n",
        "\r\n",
        "    # df['important_words'] = df['important_words'].apply(lambda x : splitWords(x))\r\n",
        "    df['important_words'] = df['important_words'].apply(lambda x : splitWords(x)) + ' ' + df['aspect_term']\r\n",
        "\r\n",
        "    # Define a corpus for the Bag of Words Model\r\n",
        "    corpus = list()\r\n",
        "    for x in df['important_words']:\r\n",
        "        corpus.append(x)\r\n",
        "\r\n",
        "    # Bag of Words\r\n",
        "    # cv = CountVectorizer(max_features=20000)\r\n",
        "    # TF-IDF\r\n",
        "    cv = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\r\n",
        "    overall_sentiment = [TextBlob(sentence).sentiment.polarity for sentence in df['text']]\r\n",
        "\r\n",
        "    # Adding overall sentiment\r\n",
        "    X = np.concatenate(\r\n",
        "        ((cv.fit_transform(corpus).toarray()), np.asarray(overall_sentiment).reshape(len(overall_sentiment), 1)), 1)\r\n",
        "    Y = None\r\n",
        "    if not output:\r\n",
        "        Y = df.iloc[:, 4].values\r\n",
        "    return df, X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI77yLJHp5GE"
      },
      "source": [
        "def returnDatasetInfo(df):\r\n",
        "\r\n",
        "    # Return the basic structure info about the dataset\r\n",
        "    print(\"Shape \\n{}\\n\\n\".format(df.shape))\r\n",
        "    print(\"Info \\n{}\\n\\n\".format(df.info()))\r\n",
        "    print(\"Description \\n{}\\n\\n\".format(df.describe()))\r\n",
        "    print(\"Missing values check \\n{}\\n\\n\".format(df.isnull().any()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2StehTG_p6hL"
      },
      "source": [
        "df=pd.read_csv('/content/English_final.csv')\r\n",
        "df['example_id']=df['Unnamed: 0']\r\n",
        "df.drop(['Unnamed: 0'],axis=1,inplace=True)\r\n",
        "#returnDatasetInfo(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2MPimISrSJ7"
      },
      "source": [
        "import spacy\r\n",
        "nlp=spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPqm8V5TrtEG"
      },
      "source": [
        "def entity_sentence_pair(data):\r\n",
        "  sent_lis=[]\r\n",
        "  ent_lis=[]\r\n",
        "  ent_loc=[]\r\n",
        "  example_id=[]\r\n",
        "  count=0\r\n",
        "  for i in range(len(data)):\r\n",
        "    text=data['Tweet'][i]\r\n",
        "    doc=nlp(text)\r\n",
        "    if(len(doc.ents)==0):\r\n",
        "      count+=1\r\n",
        "    for ent in doc.ents:\r\n",
        "      if(ent.label_=='ORG'or ent.label_=='PRODUCT'):\r\n",
        "        sent_lis.append(data['Tweet'][i])\r\n",
        "        ent_lis.append(ent.text)\r\n",
        "        ent_loc.append(str(ent.start_char)+'--'+str(ent.end_char))\r\n",
        "        example_id.append(data['example_id'][i])\r\n",
        "  #print(count)\r\n",
        "  df=pd.DataFrame()\r\n",
        "  df['example_id']=example_id\r\n",
        "  df['text']=sent_lis\r\n",
        "  df['aspect_term']=ent_lis\r\n",
        "  df['term_location']=ent_loc\r\n",
        "  return(df)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_9kIPrPzBbe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBxIouZ6pOC5"
      },
      "source": [
        "data=entity_sentence_pair(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jw06qG4t8T-"
      },
      "source": [
        "def preprocessData(df):\r\n",
        "\r\n",
        "    # Remove unnecessary stuff\r\n",
        "    for x in df['text']:\r\n",
        "        x = re.sub('[^a-zA-Z]', ' ', x)\r\n",
        "    for x in df['aspect_term']:\r\n",
        "        x = re.sub('[^a-zA-Z]', ' ', x)\r\n",
        "\r\n",
        "    # Make all the capital letters small\r\n",
        "    df['text'] = df['text'].str.lower()\r\n",
        "    df['aspect_term'] = df['aspect_term'].str.lower()\r\n",
        "\r\n",
        "    # Remove [comma] from the column df[' text']\r\n",
        "    df['text'] = df['text'].replace(\"comma\", \"\", regex=True)\r\n",
        "    df['text'] = df['text'].replace(\"\\[]\", \"\", regex=True)\r\n",
        "\r\n",
        "    # Remove [comma] from the column df['aspect_term']\r\n",
        "    df['aspect_term'] = df['aspect_term'].replace(\"comma\", \"\", regex=True)\r\n",
        "    df['aspect_term'] = df['aspect_term'].replace(\"\\[]\", \"\", regex=True)\r\n",
        "\r\n",
        "    # Remove _ from the text\r\n",
        "    df['text'] = df['text'].replace('_', '', regex=True)\r\n",
        "    df['aspect_term'] = df['aspect_term'].replace('_', '', regex=True)\r\n",
        "\r\n",
        "    # Remove special characters from text\r\n",
        "    df['text'] = df['text'].apply(lambda x: re.sub('\\W+', ' ', x))\r\n",
        "    df['aspect_term'] = df['aspect_term'].apply(lambda x: re.sub('\\W+', ' ', x))\r\n",
        "\r\n",
        "    # Remove the stop words\r\n",
        "    # nltk.download()\r\n",
        "    stopWords = set(stopwords.words(\"english\"))\r\n",
        "    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in (stopWords)]))\r\n",
        "\r\n",
        "    # Tag the words\r\n",
        "    # Each word is tagged with its type eg. Adjective, Noun, etc\r\n",
        "    # Chunk them together and return\r\n",
        "    def tagWords(sentence):\r\n",
        "        words = word_tokenize(sentence)\r\n",
        "        tagged = nltk.pos_tag(words)\r\n",
        "        return tagged\r\n",
        "\r\n",
        "    df['tagged_words'] = df['text'].apply(lambda row: tagWords(row))\r\n",
        "\r\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMyLQU57swKA",
        "outputId": "d04b939c-ff96-4cea-eaca-07e5e3e63393"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "data=preprocessData(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfoD7q77z2OJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "M_DiyPYiti9x",
        "outputId": "eebb01b5-36e3-402e-93a0-23c34e529c6d"
      },
      "source": [
        "aspectAnalysis(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-84a9c9ceb24d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maspectAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-0c52f2a63fe1>\u001b[0m in \u001b[0;36maspectAnalysis\u001b[0;34m(df, output)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Find the aspect term's first word's index in row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0maspectSplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0;31m# print('Matched Word is ', row[i][0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0maspectIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6hClHLybCcH"
      },
      "source": [
        "# Read two train datasets\r\n",
        "    df_comp_in = pd.read_csv('data/data-2_train.csv', sep='\\s*,\\s*')\r\n",
        "    df_comp_out = pd.read_csv('data/Data-2_test.csv', sep='\\s*,\\s*')\r\n",
        "    \r\n",
        "    # Your output file name\r\n",
        "    outFile = \"/content/output.txt\"\r\n",
        "\r\n",
        "    df_comp_out['class'] = np.ones(len(df_comp_out))\r\n",
        "\r\n",
        "    df = pd.concat([df_comp_in, df_comp_out])\r\n",
        "    df = preprocessData(df)\r\n",
        "    \r\n",
        "    df, X, Y = aspectAnalysis(df)\r\n",
        "    X_train = X[0:len(df_comp_in)]\r\n",
        "    Y_train = Y[0:len(df_comp_in)]\r\n",
        "    X_test = X[len(df_comp_in):]\r\n",
        "    \r\n",
        "    # Classifier\r\n",
        "    classfier_comp = trainBestClassifier(X_train, Y_train)\r\n",
        "    Y_test = classfier_comp.predict(X_test)\r\n",
        "    printOutput(df_comp_out, Y_test, outFile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibIwdg3LbMAR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOkvQi0GbbMX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-ODCQAbbiWm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BEHDGzxcBN1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ywvqCGccGyt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}